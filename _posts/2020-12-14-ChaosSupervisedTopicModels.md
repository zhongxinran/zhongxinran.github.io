---
title: 主题模型 | Supervised Topic Models（包含推导证明、理解、注释等）
author: 钟欣然
date: 2020-12-12 00:44:00 +0800
categories: [杂谈, 主题模型]
math: true
mermaid: true
---

**摘要：** sLDA（supervised latent Dirichlet allocation）是针对标记文档的主题模型，该模型采用变分EM算法，适用于多种响应变量（response variable），优于lasso和先使用无监督主题模型再进行回归。

# 1. 背景

现在有很多大型语料库，需要建立合适的统计模型对其进行分析，如基于主题的分层统计模型LDA等，但现在的主题模型大都是无监督的，只对文档中的单词进行建模，最大化其后验概率。

本文关注有响应变量的文档，如电影评论有数字作为评级、论文有下载次数，对其建立监督主题模型，和无监督主题模型主要用于分类（降维）不同，sLDA主要用于预测。

# 2. 模型

主题模型中每个文档被表示为单词$w_{1:n}$的集合，我们将文档看成是一组潜在主题产生的单词，即词汇上的一组未知分布。语料库中的文档共享相同的K个主题，但是不同的文档有不同的主题比例。在LDA中，我们从狄利克雷分布中抽取主题比例，然后从这些比例中抽取一个主题，再从相应的主题中抽取一个单词，重复$n$次构成一个文档。

在sLDA中，我们在LDA的基础上，对每个文档添加一个响应变量，对文档和响应变量共同建模，以便找到潜在的主题，从而最好地预测未标记文档的响应变量。

sLDA使用与广义线性模型相同的概率机制来适应各种类型的响应变量：无约束实值、被约束为正的实值（例如故障时间）、有序或无序的类标签、非负整数（例如计数数据）等。

模型参数：
- $K$个主题$\beta_{1:K}$，$\beta_k$是第k个主题中每个词出现概率的向量
- 狄利克雷参数$\alpha$
- 响应参数$\eta,\delta$

在sLDA中，每个文档和响应变量都来自以下生成过程：

1. 抽取主题比例$\theta \vert \alpha \sim \operatorname{Dir}(\alpha)$
2. 对于每一个词：
	
	（a）抽取主题$z_{n} \vert \theta \sim \operatorname{Mult}(\theta)$
	
	（b）抽取单词$w_{n} \vert z_{n}, \beta_{1: K} \sim \operatorname{Mult}\left(\beta_{z_{n}}\right)$
	
3. 抽取响应变量$y \vert z_{1: N}, \eta, \delta \sim \operatorname{GLM}(\bar{z}, \eta, \delta)$，其中$\bar{z}=\frac1N\sum_{n=1}^{N} z_{n}$

图示如下：

![Alt](https://img-blog.csdnimg.cn/20191222201838726.png#pic_center =500x252)
<center>图1 sLDA的图形表示 </center><br>

响应变量的分布为广义线性模型

$$
p\left(y \vert z_{1: N}, \eta, \delta\right)=h(y, \delta) \exp \left\{\frac{\left(\eta^{\top} \bar{z}\right) y-A\left(\eta^{\top} \bar{z}\right)}{\delta}\right\} \tag 1
$$

> $h(y,\delta)$为潜在测度，$\eta^{\top}\bar z$为自然参数，$\delta$为分散参数（为对$y$的方差建模提供了灵活性），$A(\eta^{\top}\bar z)$对数规范化因子

GLM框架为我们提供了对不同类型的响应变量建模的灵活性，只要响应变量的分布可以被写成上式指数分散族（exponential dispersion family）的形式。包括很多常用分布，如正态分布（适用于实值响应变量）、二项分布（适用于二项响应变量）、多项分布（适用于分类响应变量）、泊松分布和负二项分布（适用于计数响应变量）、伽马分布和威布尔分布和逆高斯分布（适用于故障时间数据）等。每个分布都对应于特定的$h(y, \delta)$和$A\left(\eta^{\top} \bar{z}\right)$。

sLDA与通常的GLM的区别在于协变量是文档中主题的经验频率，这些经验频率是不能直接观测到的。在生成过程中，这些潜在的变量负责生成文档的单词，因此响应变量和单词得以联系在一起。回归系数记为$\eta$。注意到GLM中通常包含截距项，相当于添加一个恒等于1的协变量，而在sLDA中，这一项是多余的，因为$\bar z$的各分量和恒为1。

对主题的经验频率而不是主题比例$\theta$进行回归，前者将响应变量和单词视为是不可交换的（exchangeable），首先在全部单词可交换的条件下生成文档（单词及其主题分配），然后基于该文档生成响应变量，后者将响应变量和单词视为是可交换的。前者更加合理，因为响应变量取决于文档中实际出现的主题频率，而不是产生主题的分布，如果主题数足够多，在后者中允许一些主题被完全用来解释响应变量，另一些主题被完全用来解释单词的出现，这降低了预测的性能，而在前者中，决定响应变量的潜在变量和决定单词出现的潜在变量是相同的。~~这个模型不能推断用于解释响应变量的主题集合，也不能用它来解释一些观测到的单词。

